# Item Pipeline

Responseï¼š
Spider -> Engine -> Item Pipeline

Item Pipelineçš„ä¸»è¦åŠŸèƒ½

* æ¸…æ´—HTMLæ•°æ®
* æ£€æŸ¥çˆ¬å–å­—æ®µ
* æŸ¥é‡ä¸¢å¼ƒé‡å¤å†…å®¹
* æŒä¹…åŒ–å­˜å‚¨

è‡ªå®šä¹‰Item Pipelineå¯ä»¥é€šè¿‡ä¸‹é¢æ–¹æ³•æ¥æ‰©å±•åŠŸèƒ½

* process_item(item, spider)

å¿…é¡»å®ç°è¿™ä¸ªæ–¹æ³•ï¼Œé»˜è®¤ä½¿ç”¨è¯¥æ–¹æ³•å¯¹Itemè¿›è¡Œå¤„ç†ï¼Œå¦‚è¿›è¡Œæ•°æ®å¤„ç†æˆ–å°†æ•°æ®å†™å…¥æ•°æ®åº“

è¿”å›Itemå¯¹è±¡æˆ–æŠ›å‡ºDropItemå¼‚å¸¸

è‹¥è¿”å›Itemå¯¹è±¡ï¼Œæ­¤Itemä¼šè¢«ä½ä¼˜å…ˆçº§çš„Item Pipelineå¤„ç†

è‹¥æŠ›å‡ºDropItemå¼‚å¸¸ï¼Œæ­¤Itemç›´æ¥è¢«æŠ›å¼ƒ

* open_spider(spider)

åœ¨Spiderå¼€å¯æ—¶è¢«è‡ªåŠ¨è°ƒç”¨ï¼Œå¯ä»¥åšä¸€äº›åˆå§‹åŒ–æ“ä½œï¼Œå¦‚å¼€å¯æ•°æ®åº“è¿æ¥

* close_spider(spider)

åœ¨Spiderå…³é—­æ—¶è¢«è‡ªåŠ¨è°ƒç”¨ï¼Œå¯ä»¥åšä¸€äº›æ”¶å°¾æ“ä½œï¼Œå¦‚å…³é—­æ•°æ®åº“è¿æ¥

* from_crawler(cls, crawler)

æ˜¯ä¸€ä¸ªç±»æ–¹æ³•ï¼Œç”¨@classmethodæ ‡è¯†ã€‚é€šè¿‡crawlerå¯ä»¥è·å–Scrapyçš„æ‰€æœ‰æ ¸å¿ƒç»„ä»¶ï¼Œå¦‚å…¨å±€é…ç½®

å‚æ•°clså°±æ˜¯Classï¼Œè¯¥æ–¹æ³•è¿”å›ä¸€ä¸ªClasså®ä¾‹

# å®æˆ˜

ğŸªTarget: https://ssr1.scrape.center/

:four_leaf_clover:Data Storage: MongoDB

```python
# ssr1.py
import scrapy
from scrapy import Request
from movie.items import MovieItem


class Ssr1Spider(scrapy.Spider):
    name = 'ssr1'
    allowed_domains = ['ssr1.scrape.center']
    start_url = 'http://ssr1.scrape.center'
    max_page = 10

    def start_requests(self):
        for i in range(1, self.max_page + 1):
            url = f'{self.start_url}/page/{i}'
            yield Request(url=url, callback=self.parse_index)

    def parse_index(self, response):
        for item in response.css('.item'):
            href = item.css('.name::attr(href)').extract_first()
            url = response.urljoin(href)
            yield Request(url=url, callback=self.parse_detail)

    def parse_detail(self, response):
        item = MovieItem()
        item['name'] = response.css('h2::text').extract_first()
        item['categories'] = response.css('.categories span::text').extract()
        item['score'] = response.css('.score::text').extract_first().strip()
        item['drama'] = response.css('.drama p::text').extract_first().strip()
        item['directors'] = response.css('.directors .name::text').extract()
        item['actors'] = response.css('.actors .name::text').extract()
        yield item
```

```python
# pipelines.py
import pymongo
class MongoDBPipeline(object):

    def __init__(self, connection, database, collection):
        self.connection = connection
        self.database = database
        self.collection = collection

    @classmethod
    def from_crawler(cls, crawler):
        return cls(connection=crawler.settings.get('MONGODB_CONNECTION_STRING'),
                   database=crawler.settings.get('MONGODB_DATABASE'),
                   collection=crawler.settings.get('MONGODB_COLLECTION')
                   )

    def open_spider(self, spider):
        self.client = pymongo.MongoClient(self.connection)
        self.db = self.client[self.database]

    def process_item(self, item, spider):
        self.db[self.collection].update_one({
            'name': item['name']
        }, {
            '$set': dict(item)
        }, True)
        return item

    def close_spider(self, spider):
        self.client.close()
```

```python
# settings.py
ITEM_PIPELINES = {
    'movie.pipelines.MongoDBPipeline': 300,
}
MONGODB_CONNECTION_STRING = 'localhost'
MONGODB_DATABASE = 'scrapy'
MONGODB_COLLECTION = 'movie'
```

![image-20221031101006537](../images/image-20221031101006537.png)
